# Developer Quick Reference Guide

## üöÄ **QUICK START FOR NEW JOB TYPES**

This is a condensed reference for developers implementing new job types based on the perfect implementation pattern.

---

## üìã **IMPLEMENTATION CHECKLIST**

### **Pre-Implementation**
- [ ] API specification documented
- [ ] Database schema designed
- [ ] Job type code and label chosen
- [ ] Business requirements understood

### **Core Implementation (11 Steps)**
- [ ] **Step 1**: Job type configuration
- [ ] **Step 2**: API configuration management  
- [ ] **Step 3**: Database schema implementation
- [ ] **Step 4**: API client implementation
- [ ] **Step 5**: Dummy data generator
- [ ] **Step 6**: Data processor implementation
- [ ] **Step 7**: Task integration
- [ ] **Step 8**: Routing integration
- [ ] **Step 9**: Backend controller
- [ ] **Step 10**: Main backend router integration
- [ ] **Step 11**: Dashboard analytics integration

### **Post-Implementation**
- [ ] Database tables created
- [ ] API configurations initialized
- [ ] Testing completed
- [ ] Documentation updated

---

## üîß **QUICK IMPLEMENTATION COMMANDS**

### **1. Job Type Configuration**
```python
# File: admin_panel/components/job_types.py
JOB_TYPE_MAPPING = {
    "new_job_code": "User Friendly Job Label"
}
```

### **2. API Configuration**
```python
# Files: backend/tasks/api_clients.py + backend/controllers/configuration_controller.py
APIConfiguration(
    config_name="dgi_new_job_api",
    base_url="https://dev-gvt-gateway.eksad.com/dgi-api/v1.3",
    description="DGI API for New Job Data",
    is_active=True,
    timeout_seconds=30,
    retry_attempts=3
)
```

### **3. Database Schema**
```python
# Files: backend/database.py + dashboard_analytics/database.py
class NewJobData(Base):
    __tablename__ = "new_job_data"
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    dealer_id = Column(String(10), ForeignKey("dealers.dealer_id"), nullable=False)
    # Add your fields here
    fetched_at = Column(DateTime, default=datetime.utcnow)
    
    dealer = relationship("Dealer", back_populates="new_job_data")

# Update Dealer class:
class Dealer(Base):
    new_job_data = relationship("NewJobData", back_populates="dealer")
```

### **4. API Client**
```python
# File: backend/tasks/api_clients.py
class NewJobAPIClient:
    def __init__(self):
        self.config = APIConfigManager.get_api_config("dgi_new_job_api") or APIConfigManager.get_default_config()
        self.endpoint = "/newjob/read"
    
    def fetch_data(self, dealer_id: str, from_time: str, to_time: str, api_key: str, secret_key: str, **kwargs):
        # Standard implementation pattern
```

### **5. Dummy Data Generator**
```python
# File: backend/tasks/dummy_data_generators.py
def get_dummy_new_job_data(dealer_id: str, from_time: str, to_time: str, **kwargs):
    # Generate realistic test data
    return {"status": 1, "message": None, "data": records}
```

### **6. Data Processor**
```python
# File: backend/tasks/processors/new_job_processor.py
class NewJobDataProcessor(BaseDataProcessor):
    def __init__(self):
        super().__init__("new_job_code")
        self.api_client = NewJobAPIClient()
    
    def fetch_api_data(self, dealer, from_time, to_time, **kwargs):
        # API fetch implementation
    
    def process_records(self, db, dealer_id, api_data) -> int:
        # Database processing implementation
    
    def get_summary_stats(self, db, dealer_id=None):
        # Summary statistics implementation
```

### **7. Task Integration**
```python
# File: backend/tasks/data_fetcher_router.py

# Add import
from .processors.new_job_processor import NewJobDataProcessor

# Add to processors dict
"new_job_code": NewJobDataProcessor()

# Add Celery task
@celery_app.task(bind=True)
def fetch_new_job_data(self, dealer_id, from_time=None, to_time=None, **kwargs):
    return router.execute_fetch("new_job_code", dealer_id, from_time, to_time, **kwargs)

# Add to exports
__all__ = [..., 'fetch_new_job_data', 'get_new_job_processor']
```

### **8. Routing Integration**
```python
# Files: backend/controllers/jobs_controller.py + backend/job_queue_manager.py
elif request.fetch_type == "new_job_code":
    task_name = "tasks.data_fetcher_router.fetch_new_job_data"
    message = "New job data fetch job started"
    task_args = [request.dealer_id, request.from_time, request.to_time, ...]
```

### **9. Backend Controller**
```python
# File: backend/controllers/new_job_controller.py
from fastapi import APIRouter, Depends, HTTPException, Query
from database import get_db, NewJobData, Dealer

router = APIRouter(prefix="/new_job", tags=["new_job"])

@router.get("/")
async def get_new_job_data(...):
    # Standard CRUD implementation

@router.get("/summary")
async def get_new_job_summary(...):
    # Summary statistics endpoint

@router.post("/test-fetch")
async def test_new_job_fetch(...):
    # API testing endpoint
```

### **10. Main Backend Router**
```python
# File: backend/main.py

# Add import
from controllers.new_job_controller import router as new_job_router

# Include router
app.include_router(new_job_router)

# Update API info
"endpoints": {"new_job_data": "/new_job/"}
"descriptions": {"new_job": "New job data and analytics"}
```

### **11. Dashboard Analytics**
```python
# File: dashboard_analytics/dashboard_analytics.py

# Add menu option
menu_options = {"üÜï New Job": "new_job_code"}

# Add database import
from database import ..., NewJobData

# Add page routing
elif current_page == "new_job_code":
    render_new_job_data_page(selected_dealer_id)

# Implement page function
def render_new_job_data_page(dealer_id):
    # Standard dashboard implementation with pagination, search, summary
```

---

## üéØ **STANDARD PATTERNS**

### **Error Handling Pattern**
```python
try:
    # Implementation
    return success_response
except Exception as e:
    logger.error(f"Error in operation: {e}")
    return error_response
finally:
    # Cleanup (for database sessions)
    session.close()
```

### **Database Query Pattern**
```python
# Base query with joins
query = session.query(MainTable).join(Dealer)

# Apply filters
if dealer_id != "All":
    query = query.filter(MainTable.dealer_id == dealer_id)

# Apply search
if search_term:
    query = query.filter(or_(...search conditions...))

# Apply pagination
offset = (page - 1) * page_size
records = query.order_by(MainTable.fetched_at.desc()).offset(offset).limit(page_size).all()
```

### **API Response Validation Pattern**
```python
# Validate API response
if not api_response or not isinstance(api_response, dict):
    raise ValueError("Invalid API response format")

if api_response.get("status") != 1:
    error_message = api_response.get("message", "Unknown API error")
    return {"status": 0, "message": f"API Error: {error_message}", "data": []}

data = api_response.get('data', [])
if data is None:
    data = []
```

### **Dashboard Display Pattern**
```python
# Pagination controls
col1, col2, col3 = st.columns([1, 2, 1])
with col1:
    page_size = st.selectbox("Records per page", [10, 25, 50, 100], index=1)
with col3:
    page_number = st.number_input("Page", min_value=1, max_value=total_pages, value=1)

# Search functionality
search_term = st.text_input("üîç Search (...)")

# Data table display
if data:
    df = pd.DataFrame(data)
    st.dataframe(df, use_container_width=True, hide_index=True)
    
    # Summary statistics
    st.subheader("üìä Summary Statistics")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Records", total_count)
```

---

## üîç **DEBUGGING CHECKLIST**

### **Common Issues**
- [ ] **Import errors**: Check all import statements
- [ ] **Database errors**: Verify table creation and relationships
- [ ] **API errors**: Check API configuration and endpoints
- [ ] **Task errors**: Verify Celery task registration
- [ ] **Dashboard errors**: Check session management and data formatting

### **Testing Steps**
1. **Test dummy data**: Verify dummy data generation
2. **Test API client**: Test API client with test-fetch endpoint
3. **Test data processing**: Verify database storage
4. **Test dashboard**: Check data display and search
5. **Test integration**: End-to-end testing

### **Verification Commands**
```bash
# Test API configuration
curl -X POST "http://localhost:8000/api-configurations/force-reinitialize"

# Test job execution
curl -X POST "http://localhost:8000/new_job/test-fetch?dealer_id=12284&from_time=2024-01-15%2000:00:00&to_time=2024-01-16%2023:59:59"

# Check database tables
docker-compose exec backend python -c "from database import create_tables; create_tables()"
```

---

## üìä **PERFORMANCE GUIDELINES**

### **Database Optimization**
- Use proper indexes on search fields
- Implement efficient pagination
- Use joins instead of multiple queries
- Add database constraints for data integrity

### **API Optimization**
- Implement proper timeout handling
- Use connection pooling
- Add retry logic for failed requests
- Cache API configurations

### **Dashboard Optimization**
- Use Streamlit caching for expensive operations
- Implement efficient data loading
- Use proper session management
- Optimize query performance

---

## üéØ **SUCCESS CRITERIA**

A successful implementation should have:

- ‚úÖ **Zero errors** during normal operation
- ‚úÖ **Fast response times** (< 2 seconds for data display)
- ‚úÖ **Comprehensive search** across all relevant fields
- ‚úÖ **Accurate statistics** in summary displays
- ‚úÖ **Proper error handling** for all scenarios
- ‚úÖ **Consistent UX** with existing job types
- ‚úÖ **Complete test coverage** with dummy and real data

---

## üìö **REFERENCE FILES**

### **Implementation Examples**
- `backend/tasks/processors/workshop_invoice_processor.py` - Complex nested data
- `backend/tasks/processors/unpaid_hlo_processor.py` - Customer-centric data
- `backend/tasks/processors/parts_invoice_processor.py` - Financial data

### **Documentation**
- `docs/PERFECT_JOB_TYPE_IMPLEMENTATION_GUIDE.md` - Complete implementation guide
- `docs/LATEST_IMPLEMENTATION_SUMMARY.md` - System overview
- `docs/API_CONFIGURATION_MANAGEMENT_GUIDE.md` - API configuration guide

---

**This quick reference provides everything needed to implement new job types following the established perfect implementation pattern.** üöÄ
